# VidSynth 项目开发日志

**作者**：Sherwen
**项目**：Theme-driven Multi-Source Video Trimming (VidSynth)
**时间跨度**：2025.11.01 - 2025.12.05

---

## 2025-11-01 —— 2025-11-10：立项与架构蓝图

### 📅 阶段目标
确定毕设题目，明确核心痛点，完成技术路线的初步规划与可行性验证。

### 🛠 技术设计与决策
在立项之初，我面临的最大问题是“如何从海量视频中自动剪出符合特定主题的短片”。这是一个典型的多模态理解与生成问题。

1.  **北极星目标（North Star Goal）**：确立了系统的输入输出标准。输入为“主题关键词 + 视频集”，输出为“连贯短视频”。
2.  **MVP 原则（Minimum Viable Product）**：
    *   **奥卡姆剃刀**：如无必要，勿增实体。决定暂不涉及复杂的音频理解和字幕生成，确立了**“纯视觉优先”**的技术路线。
    *   **Pipeline 设计**：将复杂的任务拆解为四个线性步骤：`切分 (Segment)` -> `匹配 (Match)` -> `编排 (Sequence)` -> `导出 (Export)`。
3.  **技术选型**：
    *   语言：Python (生态最丰富)
    *   核心模型：CLIP (OpenAI/OpenCLIP)。理由是其强大的零样本（Zero-shot）图文对齐能力，能极大降低训练成本，直接利用预训练知识。

### 📝 变更与记录
*   **2025-11-10**：提交了第一版架构文档 `MVP_framework.md`，详细定义了每个 Step 的输入输出 JSON 格式。这是一个关键节点，后续的所有开发都严格遵循了这个契约。

### 💭 心路历程
> 万事开头难。刚开始构思时，脑子里有很多“高大上”的想法，比如用视频大模型（Video-LLM）、做自动配音、自动特效等等。但指导老师提醒我，毕设首先要“跑通”，其次才是“跑好”。
>
> 于是我强迫自己做减法。当我写下“MVP原则：纯视觉优先”这一行字时，心里踏实了很多。不再追求大而全，而是专注于把“根据文字找画面”这一件事做到极致。`MVP_framework.md` 的完成让我感觉项目有了骨架，不再是空中楼阁。

---

## 2025-11-11 —— 2025-11-14：迈出第一步 —— 视频切分 (Step 1)

### 📅 阶段目标
实现 `segment` 模块，将长视频自动化切分为具有独立语义的短片段（Clips）。

### 🛠 技术细节
*   **切分策略**：没有采用复杂的深度学习时序检测模型，而是选择了更轻量级的 **1fps 抽帧 + 视觉 Embedding + 简易镜头分割**。
*   **算法逻辑**：
    *   计算相邻帧的 CLIP Embedding 余弦距离和直方图差异。
    *   双阈值控制：当视觉内容发生突变（如转场）时，进行切分。
*   **数据结构**：设计了包含 `vis_emb_avg`（视觉特征均值）的 JSON 结构，这样特征提取一次后可以永久复用，后续的主题匹配不需要再读视频文件，极大提高了效率。

### 📝 变更与记录
*   **2025-11-13**：完成了 `segment-video` CLI 命令的开发，并在 `PROGRESS.md` 中标记 Step1 为已完成。
*   **2025-11-14 (Fix)**：在测试中发现，如果视频极短或者切分阈值设置不当，会出现“零时长片段”的 Bug。紧急修复了 `clipper` 逻辑，增加了最小帧数校验。

### 💭 心路历程
> 第一行代码总是最难写的。在处理视频帧时，我一度纠结 1fps 的采样率是不是太低了？会不会漏掉关键动作？但转念一想 MVP 原则，先跑起来再说。
>
> 实际运行效果出乎意料地好，CLIP 的特征非常鲁棒，即使是每秒一帧也能很好地概括画面内容。看到终端里打印出一个个 JSON 格式的片段信息时，有一种“系统活了”的兴奋感。14号那个 Bug 让我意识到工程实现的严谨性，理论上完美的设计在脏数据面前往往不堪一击。

---

## 2025-11-15 —— 2025-11-18：赋予灵魂 —— 主题匹配 (Step 2)

### 📅 阶段目标
实现 `theme_match` 模块，让系统能够“听懂”用户想要什么主题，并给片段打分。

### 🛠 技术细节
*   **零样本匹配**：利用 CLIP 的 Text Encoder 将用户输入的“主题”（如“海滩”）转换为向量，与片段的 `vis_emb_avg` 计算相似度。
*   **正负样本对照机制**：
    *   这是本阶段的一个亮点设计。单纯计算“与A的相似度”容易产生误判（比如把所有亮色的画面都当成海滩）。
    *   引入了 `S_pos` (正向提示词得分) 和 `S_neg` (负向/对照组提示词得分)。
    *   最终得分 `Score = S_pos - S_neg`。这极大地提高了匹配的准确性，有效过滤了似是而非的片段。

### 📝 变更与记录
*   **2025-11-18**：发布 `match-theme` 命令。接入了 Deepseek 来辅助生成更丰富的 Prompt 原型（Prompt Engineering），弥补了用户输入词汇单一的问题。

### 💭 心路历程
> 这一步是项目的核心。最开始测试时，我输入“猫”，结果系统把“老虎”甚至“毛绒玩具”都选进来了。这让我意识到 CLIP 虽然强，但需要引导。
>
> 引入“对照组”的概念是受到对比学习的启发。当我加上“非猫”的负向描述后，排序结果瞬间精准了很多。这种通过算法设计优化模型表现的过程，让我深刻体会到了算法工程师的乐趣——不是单纯调包，而是理解模型特性并利用它。

---

## 2025-11-19 —— 2025-11-27：闭环 —— 编排与导出 (Step 3 & 4)

### 📅 阶段目标
实现 `sequencer` 和 `exporter`，将打好分的片段拼接成最终的 MP4 视频，完成 MVP 闭环。

### 🛠 技术细节
*   **迟滞阈值筛选**：在筛选片段时，设置了上/下两个阈值，避免片段在临界值附近反复横跳，保证选出的片段序列在时间上的连贯性。
*   **EDL (Edit Decision List)**：在导出前生成一个中间层的 JSON EDL 文件。这使得我可以在真正渲染视频前，人工检查剪辑逻辑是否合理，也方便后续人工微调。
*   **FFmpeg 集成**：使用 `ffmpeg-python` 进行实际的视频处理。实现了音频的 Fade-in/Fade-out，避免拼接处的爆音，这是一个极小但极其影响体验的细节。

### 📝 变更与记录
*   **2025-11-20**：初步实现导出功能。
*   **2025-11-27**：重构了导出模块。增加了临时文件自动清理和更完善的错误处理机制。因为在处理大视频时，磁盘空间和内存管理成了瓶颈，必须优化。

### 💭 心路历程
> 当第一个完整的 Demo 视频生成出来的时候，虽然只有短短 30 秒，而且转场还有点生硬，但我把它看了好几遍。从一堆杂乱的原始素材，到一段有主题的视频，整个流程全自动完成，这种成就感是无与伦比的。
>
> 后期的优化工作（27号）比较枯燥，都是些异常处理和资源管理，但这些才是让 Demo 变成可用系统的关键。我学会了敬畏工程细节。

---

## 2025-11-28 —— 2025-12-05：探索未知 —— 聚类与未来

### 📅 阶段目标
MVP 功能稳定后，开始思考如何处理“用户不知道自己想要什么主题”的场景，探索视频聚类功能。

### 🛠 技术细节
*   **无监督聚类**：开发 `cluster` 模块，尝试对视频库中的所有片段进行 K-Means 或 DBSCAN 聚类，自动发现潜在的主题集合。
*   **测试数据生成**：编写了生成测试数据的脚本，用于验证聚类算法的有效性。

### 📝 变更与记录
*   **2025-12-01**：提交了聚类模块的初版代码。目前还在调试不同聚类算法对 CLIP 特征的适应性。

### 💭 心路历程
> 现在的系统已经能用了，但我希望能更进一步。如果用户不需要输入关键词，系统能自己告诉用户“你的相册里主要有这几类内容”，那体验会更好。
>
> 这也是从“工具”向“智能助手”跨越的一步。虽然目前聚类效果还不够完美（CLIP 向量在高维空间的分布比想象中复杂），但这将是我毕设接下来的主要攻坚方向。回首这一个月，从 0 到 1 的过程虽然辛苦，但每一步都算数。

---
